trainer:
  _target_: sampler.gfn.trainer.trajectory_wise_trainer.OffPolicyTrainer

epochs: 25000
batch_size: 300

fwd_loss: tb
bwd_loss: tb

exploratory: true
exploration_factor: 0.2
exploration_wd: true

buffer:
  buffer_size: 600000
  # 300 * 1000 * 2
  batch_size: ${train.batch_size}
  prioritized: "rank"
  rank_weight: 1e-2
  device: ${device}
  beta: 1.0

local_search:
  max_iter_ls: 200
  burn_in: 100
  ls_cycle: 100
  ld_step: 0.1
  ld_schedule: true
  target_acceptance_rate: 0.574

num_backward_train_per_forward_train: 1