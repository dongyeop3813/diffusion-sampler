trainer:
  _target_: trainer.trajectory_wise_trainer.OffPolicyTrainer

epochs: 25000
batch_size: 300

fwd_loss: tb
bwd_loss: tb

exploratory: true
exploration_factor: 0.2
exploration_wd: true

optimizer:
  lr_policy: 1e-3
  lr_flow: 1e-1
  lr_back: 1e-3
  weight_decay: 1e-7
  use_weight_decay: false

buffer:
  buffer_size: 600000
  # 300 * 1000 * 2
  batch_size: ${train.batch_size}
  prioritized: true
  rank_weight: 1e-2
  device: ${device}
  beta: 1.0

local_search:
  max_iter_ls: 200
  burn_in: 100
  ls_cycle: 100
  ld_step: 0.1
  ld_schedule: true
  target_acceptance_rate: 0.574
